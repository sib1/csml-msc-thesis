{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BertPretrain.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMPv0_33SI1J",
        "colab_type": "code",
        "outputId": "2d4f6290-63ce-4232-d2e4-0ef764a806ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!nvidia-smi\n",
        "\n",
        "!pip install pytorch-nlp\n",
        "!pip install pytorch-transformers\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jul 31 07:29:54 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   69C    P0    32W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Collecting pytorch-nlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/ae/b6d18c3f37da5a78e83701469e6153811f4b0ecb3f9387bb3e9a65ca48ee/pytorch_nlp-0.4.1-py3-none-any.whl (82kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (0.24.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (1.16.4)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->pytorch-nlp) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas->pytorch-nlp) (2.5.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (2019.6.16)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (3.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas->pytorch-nlp) (1.12.0)\n",
            "Installing collected packages: pytorch-nlp\n",
            "Successfully installed pytorch-nlp-0.4.1\n",
            "Collecting pytorch-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/b5/2d78e74001af0152ee61d5ad4e290aec9a1e43925b21df2dc74ec100f1ab/pytorch_transformers-1.0.0-py3-none-any.whl (137kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.28.1)\n",
            "Collecting sentencepiece (from pytorch-transformers)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/95/7f357995d5eb1131aa2092096dca14a6fc1b1d2860bd99c22a612e1d1019/sentencepiece-0.1.82-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 51.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.9.189)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.16.4)\n",
            "Collecting regex (from pytorch-transformers)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/4e/1b178c38c9a1a184288f72065a65ca01f3154df43c6ad898624149b8b4e0/regex-2019.06.08.tar.gz (651kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 48.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.1.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2019.6.16)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.189 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (1.12.189)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.2.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.189->boto3->pytorch-transformers) (2.5.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.189->boto3->pytorch-transformers) (0.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.189->boto3->pytorch-transformers) (1.12.0)\n",
            "Building wheels for collected packages: regex\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/e4/80/abf3b33ba89cf65cd262af8a22a5a999cc28fbfabea6b38473\n",
            "Successfully built regex\n",
            "Installing collected packages: sentencepiece, regex, pytorch-transformers\n",
            "Successfully installed pytorch-transformers-1.0.0 regex-2019.6.8 sentencepiece-0.1.82\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SABdpWjYSLvJ",
        "colab_type": "code",
        "outputId": "ada8b801-f0de-4db3-dde0-ef4570e8782e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "source": [
        "#Get Data from Dropbox\n",
        "!git clone https://github.com/thatbrguy/Dropbox-Uploader.git\n",
        "%cd Dropbox-Uploader\n",
        "!chmod +x dropbox_uploader.sh\n",
        "!bash dropbox_uploader.sh\n",
        "!echo \"CHTZbyeOjyYAAAAAAAAAfZTfC_d3kTmHXSqg7-941o9Nolj-FEAJbjyUKutEtCSu\" > token.txt\n",
        "!bash dropbox_uploader.sh\n",
        "\n",
        "!bash dropbox_uploader.sh download imdb_masked_lm.zip\n",
        "!unzip imdb_masked_lm.zip\n",
        "!rm imdb_masked_lm.zip\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Dropbox-Uploader'...\n",
            "remote: Enumerating objects: 951, done.\u001b[K\n",
            "remote: Total 951 (delta 0), reused 0 (delta 0), pack-reused 951\u001b[K\n",
            "Receiving objects: 100% (951/951), 309.19 KiB | 221.00 KiB/s, done.\n",
            "Resolving deltas: 100% (505/505), done.\n",
            "/content/Dropbox-Uploader\n",
            "\n",
            " This is the first time you run this script, please follow the instructions:\n",
            "\n",
            " 1) Open the following URL in your Browser, and log in using your account: https://www.dropbox.com/developers/apps\n",
            " 2) Click on \"Create App\", then select \"Dropbox API app\"\n",
            " 3) Now go on with the configuration, choosing the app permissions and access restrictions to your DropBox folder\n",
            " 4) Enter the \"App Name\" that you prefer (e.g. MyUploader245411274017848)\n",
            "\n",
            " Now, click on the \"Create App\" button.\n",
            "\n",
            " When your new App is successfully created, please click on the Generate button\n",
            " under the 'Generated access token' section, then execute the following command:\n",
            "\n",
            " echo \"INPUT_YOUR_ACCESS_TOKEN_HERE\" > token.txt\n",
            "\n",
            "\n",
            " This is the first time you run this script, please follow the instructions:\n",
            "\n",
            " 1) Open the following URL in your Browser, and log in using your account: https://www.dropbox.com/developers/apps\n",
            " 2) Click on \"Create App\", then select \"Dropbox API app\"\n",
            " 3) Now go on with the configuration, choosing the app permissions and access restrictions to your DropBox folder\n",
            " 4) Enter the \"App Name\" that you prefer (e.g. MyUploader15090867324054)\n",
            "\n",
            " Now, click on the \"Create App\" button.\n",
            "\n",
            " When your new App is successfully created, please click on the Generate button\n",
            " under the 'Generated access token' section, then execute the following command:\n",
            "\n",
            " echo \"INPUT_YOUR_ACCESS_TOKEN_HERE\" > token.txt\n",
            "\n",
            "\n",
            " Found token.txt with access token: CHTZbyeOjyYAAAAAAAAAfZTfC_d3kTmHXSqg7-941o9Nolj-FEAJbjyUKutEtCSu\n",
            "\n",
            " You can unlink this account if the token is wrong, using:\n",
            " ./dropbox-uploader.sh unlink\n",
            "\n",
            " The configuration has been saved.\n",
            " > Downloading \"/kindle_reviews_128.zip\" to \"/content/Dropbox-Uploader/kindle_reviews_128.zip\"... DONE\n",
            "Archive:  kindle_reviews_128.zip\n",
            "   creating: kindle_reviews_128/\n",
            "  inflating: kindle_reviews_128/kindle_ids_128  \n",
            "  inflating: kindle_reviews_128/kindle_input_masks_128  \n",
            "  inflating: kindle_reviews_128/kindle_masked_labels_128  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TyUcD-ISYFS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from pytorch_transformers import BertForMaskedLM, AdamW, BertConfig\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from random import Random\n",
        "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "simxoDChSqdw",
        "colab_type": "code",
        "outputId": "387b9de1-f09b-4df5-d4c7-78b50e8b5934",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "config = BertConfig.from_pretrained('bert-base-uncased')\n",
        "config.output_hidden_states=True\n",
        "model = BertForMaskedLM(config)\n",
        "#model.load_state_dict(torch.load(\"checkpoint_6000\"))\n",
        "#model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "#model = torch.load('pretrained_bert_imdb_final')\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 313/313 [00:00<00:00, 51259.89B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForMaskedLM(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (cls): BertOnlyMLMHead(\n",
              "    (predictions): BertLMPredictionHead(\n",
              "      (transform): BertPredictionHeadTransform(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "      )\n",
              "      (decoder): Linear(in_features=768, out_features=30522, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB4vqU_hS054",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load preprocessed Data\n",
        "\n",
        "masked_input_ids = torch.load('kindle_reviews_128/kindle_ids_128')\n",
        "attention_masks = torch.load('kindle_reviews_128/kindle_input_masks_128')\n",
        "masked_lm_labels = torch.load('kindle_reviews_128/kindle_masked_labels_128')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUGmWSScThkS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_batch_size = 50\n",
        "\n",
        "#Load data into dataloader\n",
        "train_data = TensorDataset(masked_input_ids, attention_masks, masked_lm_labels)\n",
        "train_sampler = SequentialSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc6Q61O4Tyyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare optimizer\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "    \n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmVJospgT3Ma",
        "colab_type": "code",
        "outputId": "68628f01-5fb2-411a-bd8c-8b2bb30f5bf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "max_grad_norm = 1.0\n",
        "# TRAIN loop\n",
        "model.train()\n",
        "\n",
        "epochs = 1\n",
        "\n",
        "\n",
        "for j in range(epochs):\n",
        "  \n",
        "  print(\"Begin Training\")\n",
        "  \n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    \n",
        "        \n",
        "    if(step%100==0):\n",
        "      print(\"batch: \", step+1)\n",
        "\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_train_ids, b_input_masks, b_train_labels = batch\n",
        "\n",
        "    # forward pass\n",
        "    outputs = model(input_ids=b_train_ids,attention_mask=b_input_masks,masked_lm_labels=b_train_labels)\n",
        "    loss = outputs[0]\n",
        "\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # gradient clipping\n",
        "    torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "    model.zero_grad()\n",
        "\n",
        "    #Save model every 2000 batches:\n",
        "\n",
        "    if((step)%2000==0):\n",
        "      torch.save(model,'pretrained_bert_kindle_128_' + str(step))\n",
        "       \n",
        "\n",
        "     "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Begin Training\n",
            "batch:  1\n",
            "batch:  101\n",
            "batch:  201\n",
            "batch:  301\n",
            "batch:  401\n",
            "batch:  501\n",
            "batch:  601\n",
            "batch:  701\n",
            "batch:  801\n",
            "batch:  901\n",
            "batch:  1001\n",
            "batch:  1101\n",
            "batch:  1201\n",
            "batch:  1301\n",
            "batch:  1401\n",
            "batch:  1501\n",
            "batch:  1601\n",
            "batch:  1701\n",
            "batch:  1801\n",
            "batch:  1901\n",
            "batch:  2001\n",
            "batch:  2101\n",
            "batch:  2201\n",
            "batch:  2301\n",
            "batch:  2401\n",
            "batch:  2501\n",
            "batch:  2601\n",
            "batch:  2701\n",
            "batch:  2801\n",
            "batch:  2901\n",
            "batch:  3001\n",
            "batch:  3101\n",
            "batch:  3201\n",
            "batch:  3301\n",
            "batch:  3401\n",
            "batch:  3501\n",
            "batch:  3601\n",
            "batch:  3701\n",
            "batch:  3801\n",
            "batch:  3901\n",
            "batch:  4001\n",
            "batch:  4101\n",
            "batch:  4201\n",
            "batch:  4301\n",
            "batch:  4401\n",
            "batch:  4501\n",
            "batch:  4601\n",
            "batch:  4701\n",
            "batch:  4801\n",
            "batch:  4901\n",
            "batch:  5001\n",
            "batch:  5101\n",
            "batch:  5201\n",
            "batch:  5301\n",
            "batch:  5401\n",
            "batch:  5501\n",
            "batch:  5601\n",
            "batch:  5701\n",
            "batch:  5801\n",
            "batch:  5901\n",
            "batch:  6001\n",
            "batch:  6101\n",
            "batch:  6201\n",
            "batch:  6301\n",
            "batch:  6401\n",
            "batch:  6501\n",
            "batch:  6601\n",
            "batch:  6701\n",
            "batch:  6801\n",
            "batch:  6901\n",
            "batch:  7001\n",
            "batch:  7101\n",
            "batch:  7201\n",
            "batch:  7301\n",
            "batch:  7401\n",
            "batch:  7501\n",
            "batch:  7601\n",
            "batch:  7701\n",
            "batch:  7801\n",
            "batch:  7901\n",
            "batch:  8001\n",
            "batch:  8101\n",
            "batch:  8201\n",
            "batch:  8301\n",
            "batch:  8401\n",
            "batch:  8501\n",
            "batch:  8601\n",
            "batch:  8701\n",
            "batch:  8801\n",
            "batch:  8901\n",
            "batch:  9001\n",
            "batch:  9101\n",
            "batch:  9201\n",
            "batch:  9301\n",
            "batch:  9401\n",
            "batch:  9501\n",
            "batch:  9601\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5VdJp2hVJ4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2d19stlNvMJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model,'pretrained_bert_kindle_128_final')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84JfWdfTNwZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}